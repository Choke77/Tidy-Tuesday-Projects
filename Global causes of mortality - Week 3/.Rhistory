# remove blank spaces at the beginning
x = gsub("^ ", "", x)
# remove blank spaces at the end
x = gsub(" $", "", x)
return(x)
}
docs <- clean.text(toSpace)
remwords <- c("tco")
docs <- tm_map(docs, removeWords, c(stopwords("english")))
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
docs <- Corpus(VectorSource(tweets.txt))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
clean.text = function(x)
{
# tolower
x = tolower(x)
# remove rt
x = gsub("rt", "", x)
# remove at
x = gsub("@\\w+", "", x)
# remove punctuation
x = gsub("[[:punct:]]", "", x)
# remove numbers
x = gsub("[[:digit:]]", "", x)
# remove links http
x = gsub("http\\w+", "", x)
# remove tabs
x = gsub("[ |\t]{2,}", "", x)
# remove blank spaces at the beginning
x = gsub("^ ", "", x)
# remove blank spaces at the end
x = gsub(" $", "", x)
return(x)
}
docs <- clean.text(toSpace)
remwords <- c("tco", "https")
docs <- tm_map(docs, removeWords, c(stopwords("english")))
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
docs <- Corpus(VectorSource(tweets.txt))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
clean.text = function(x)
{
# tolower
x = tolower(x)
# remove rt
x = gsub("rt", "", x)
# remove at
x = gsub("@\\w+", "", x)
# remove punctuation
x = gsub("[[:punct:]]", "", x)
# remove numbers
x = gsub("[[:digit:]]", "", x)
# remove links http
x = gsub("http\\w+", "", x)
# remove tabs
x = gsub("[ |\t]{2,}", "", x)
# remove blank spaces at the beginning
x = gsub("^ ", "", x)
# remove blank spaces at the end
x = gsub(" $", "", x)
return(x)
}
docs <- clean.text(toSpace)
remwords <- c("tco", "https")
docs <- tm_map(docs, removeWords, remwords, c(stopwords("english")))
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
docs <- Corpus(VectorSource(tweets.txt))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
clean.text = function(x)
{
# tolower
x = tolower(x)
# remove rt
x = gsub("rt", "", x)
# remove at
x = gsub("@\\w+", "", x)
# remove punctuation
x = gsub("[[:punct:]]", "", x)
# remove numbers
x = gsub("[[:digit:]]", "", x)
# remove links http
x = gsub("http\\w+", "", x)
# remove tabs
x = gsub("[ |\t]{2,}", "", x)
# remove blank spaces at the beginning
x = gsub("^ ", "", x)
# remove blank spaces at the end
x = gsub(" $", "", x)
return(x)
}
docs <- clean.text(toSpace)
remwords <- c(stopwords('english'),"tco", "https")
docs <- tm_map(docs, removeWords, remwords, remwords)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
docs <- Corpus(VectorSource(tweets.txt))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
clean.text = function(x)
{
# tolower
x = tolower(x)
# remove rt
x = gsub("rt", "", x)
# remove at
x = gsub("@\\w+", "", x)
# remove punctuation
x = gsub("[[:punct:]]", "", x)
# remove numbers
x = gsub("[[:digit:]]", "", x)
# remove links http
x = gsub("http\\w+", "", x)
# remove tabs
x = gsub("[ |\t]{2,}", "", x)
# remove blank spaces at the beginning
x = gsub("^ ", "", x)
# remove blank spaces at the end
x = gsub(" $", "", x)
return(x)
}
docs <- clean.text(toSpace)
remwords <- c(stopwords('english'), 'tco', 'https')
docs <- tm_map(docs, removeWords, remwords)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
wordcloud2(d, shape = "cardioid")
letterCloud(d, word = "R")
wordcloud2(d, shape = "star", color="random-light")
wordcloud2(d, shape = "star", color="random-light", backgroundColor = "grey")
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "mauve")
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "pink")
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "light-pink")
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "lightpink")
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "lightblue")
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "lightblue", size = 2)
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "lightblue", minRotation = -pi/2, maxRotation = -pi/2)
tweets <- userTimeline("dsilvadeepal",n=3200,includeRts = FALSE)
tweets.txt <- sapply(tweets, function(t)t$getText())
tweets.txt <- str_replace_all(tweets.txt,"[^[:graph:]]", " ")
docs <- Corpus(VectorSource(tweets.txt))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
clean.text = function(x)
{
# tolower
x = tolower(x)
# remove rt
x = gsub("rt", "", x)
# remove at
x = gsub("@\\w+", "", x)
# remove punctuation
x = gsub("[[:punct:]]", "", x)
# remove numbers
x = gsub("[[:digit:]]", "", x)
# remove links http
x = gsub("http\\w+", "", x)
# remove tabs
x = gsub("[ |\t]{2,}", "", x)
# remove blank spaces at the beginning
x = gsub("^ ", "", x)
# remove blank spaces at the end
x = gsub(" $", "", x)
return(x)
}
docs <- clean.text(toSpace)
remwords <- c(stopwords('english'), 'tco', 'https')
docs <- tm_map(docs, removeWords, remwords)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
#wordcloud(vector, scale=c(6,0.5), max.words=300,
#random.order=FALSE, rot.per=0.35,colors=brewer.pal(8,"Dark2"))
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "lightblue", minRotation = -pi/2, maxRotation = -pi/2)
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "lightblue",
minRotation = -pi/2, maxRotation = -pi/2 )
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "lightblue",
minRotation = -pi/2, maxRotation = -pi/6)
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "lightblue",
minRotation = -pi/2, maxRotation = -pi/6, size = 0.5)
tweets <- userTimeline("dsilvadeepal",n=3200,includeRts = FALSE)
tweets.txt <- sapply(tweets, function(t)t$getText())
tweets.txt <- str_replace_all(tweets.txt,"[^[:graph:]]", " ")
tweets <- Corpus(VectorSource(tweets.txt))
inspect(tweets)
sep.space <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
clean.text = function(x)
{
# tolower
x = tolower(x)
# remove rt
x = gsub("rt", "", x)
# remove at
x = gsub("@\\w+", "", x)
# remove punctuation
x = gsub("[[:punct:]]", "", x)
# remove numbers
x = gsub("[[:digit:]]", "", x)
# remove links http
x = gsub("http\\w+", "", x)
# remove tabs
x = gsub("[ |\t]{2,}", "", x)
# remove blank spaces at the beginning
x = gsub("^ ", "", x)
# remove blank spaces at the end
x = gsub(" $", "", x)
return(x)
}
clean_tweet <- clean.text(sep.space)
wordsToRemove <- c(stopwords('english'), 'tco', 'https')
clean_tweet <- tm_map(clean_tweet, removeWords, wordsToRemove)
dtm <- TermDocumentMatrix(clean_tweet)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
#wordcloud(vector, scale=c(6,0.5), max.words=300,
#random.order=FALSE, rot.per=0.35,colors=brewer.pal(8,"Dark2"))
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "lightblue",
minRotation = -pi/4, maxRotation = -pi/4, size = 0.5)
#http://rstudio-pubs-static.s3.amazonaws.com/71296_3f3ee76e8ef34410a1635926f740c473.html
library(twitteR)
library(ROAuth)
library(stringr)
library(tm)
library(wordcloud2)
library(tidytext)
library(tidyverse)
library(dplyr)
consumer_key <- "mftPn6qtOfCL46fzJY3pTeVI4"
consumer_secret <- "x5D1VIlV1O4TkZPshj31j88CutI8X2eSfgryy0pdMpDJjUTcKK"
access_token <- "2924790128-WORx3u9bTwDqAYr6zSanDnDlafLaSHBdzVKYZxC"
access_secret <- "aMFTRClgP3Q2K0pJ21p1fromq1OWx4cS1hU6i4qJyqpZb"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#tweets <- searchTwitter("#rstats",n=3000,lang="en", resultType = "popular")
tweets <- userTimeline("dsilvadeepal",n=3200,includeRts = FALSE)
tweets.txt <- sapply(tweets, function(t)t$getText())
tweets.txt <- str_replace_all(tweets.txt,"[^[:graph:]]", " ")
tweets <- Corpus(VectorSource(tweets.txt))
inspect(tweets)
#sep.space <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
clean.text = function(x)
{
# tolower
x = tolower(x)
# remove rt
x = gsub("rt", "", x)
# remove at
x = gsub("@\\w+", "", x)
# remove punctuation
x = gsub("[[:punct:]]", "", x)
# remove numbers
x = gsub("[[:digit:]]", "", x)
# remove links http
x = gsub("http\\w+", "", x)
# remove tabs
x = gsub("[ |\t]{2,}", "", x)
# remove blank spaces at the beginning
x = gsub("^ ", "", x)
# remove blank spaces at the end
x = gsub(" $", "", x)
return(x)
}
#clean_tweet <- clean.text(sep.space)
clean_tweet <- clean.text(tweets)
wordsToRemove <- c(stopwords('english'), 'tco', 'https')
clean_tweet <- tm_map(clean_tweet, removeWords, wordsToRemove)
dtm <- TermDocumentMatrix(clean_tweet)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "lightblue",
minRotation = -pi/4, maxRotation = -pi/4, size = 0.5)
clean_tweet <- clean.text(tweets)
wordsToRemove <- c(stopwords('english'), 'tco', 'https')
clean_tweet <- tm_map(clean_tweet, removeWords, wordsToRemove)
dtm <- TermDocumentMatrix(clean_tweet, control = list(wordLengths = c(1, Inf)))
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "lightblue",
minRotation = -pi/4, maxRotation = -pi/4, size = 0.5)
clean_tweet <- clean.text(tweets)
wordsToRemove <- c(stopwords('en'), 'tco', 'https')
clean_tweet <- tm_map(clean_tweet, removeWords, wordsToRemove)
dtm <- TermDocumentMatrix(clean_tweet, control = list(wordLengths = c(1, Inf)))
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "lightblue",
minRotation = -pi/4, maxRotation = -pi/4, size = 0.5)
clean_tweet <- clean.text(sep.space)
#clean_tweet <- clean.text(tweets)
wordsToRemove <- c(stopwords('en'), 'tco', 'https')
clean_tweet <- tm_map(clean_tweet, removeWords, wordsToRemove)
dtm <- TermDocumentMatrix(clean_tweet, control = list(wordLengths = c(1, Inf)))
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "lightblue",
minRotation = -pi/4, maxRotation = -pi/4, size = 0.5)
tweets <- userTimeline("dsilvadeepal",n=3200,includeRts = FALSE)
tweets.txt <- sapply(tweets, function(t)t$getText())
tweets.txt <- str_replace_all(tweets.txt,"[^[:graph:]]", " ")
inspect(tweets)
#sep.space <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
clean.text = function(x)
{
# tolower
x = tolower(x)
# remove rt
x = gsub("rt", "", x)
# remove at
x = gsub("@\\w+", "", x)
# remove punctuation
x = gsub("[[:punct:]]", "", x)
# remove numbers
x = gsub("[[:digit:]]", "", x)
# remove links http
x = gsub("http\\w+", "", x)
# remove tabs
x = gsub("[ |\t]{2,}", "", x)
# remove blank spaces at the beginning
x = gsub("^ ", "", x)
# remove blank spaces at the end
x = gsub(" $", "", x)
return(x)
}
#clean_tweet <- clean.text(sep.space)
clean_tweet <- clean.text(tweets.txt)
wordsToRemove <- c(stopwords('en'), 'tco', 'https')
tweets <- Corpus(VectorSource(clean_tweet))
clean_tweet <- tm_map(clean_tweet, removeWords, wordsToRemove)
dtm <- TermDocumentMatrix(clean_tweet, control = list(wordLengths = c(1, Inf)))
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "lightblue",
minRotation = -pi/4, maxRotation = -pi/4, size = 0.5)
clean_tweet <- clean.text(tweets.txt)
wordsToRemove <- c(stopwords('en'), 'tco', 'https')
tweets <- Corpus(VectorSource(clean_tweet))
clean_tweet <- tm_map(tweets, removeWords, wordsToRemove)
dtm <- TermDocumentMatrix(clean_tweet, control = list(wordLengths = c(1, Inf)))
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "lightblue",
minRotation = -pi/4, maxRotation = -pi/4, size = 0.5)
tweets <- userTimeline("dsilvadeepal",n=3200,includeRts = FALSE)
tweets.txt <- sapply(tweets, function(t)t$getText())
tweets.txt <- str_replace_all(tweets.txt,"[^[:graph:]]", " ")
clean.text = function(x)
{
# tolower
x = tolower(x)
# remove rt
x = gsub("rt", "", x)
# remove at
x = gsub("@\\w+", "", x)
# remove punctuation
x = gsub("[[:punct:]]", "", x)
# remove numbers
x = gsub("[[:digit:]]", "", x)
# remove links http
x = gsub("http\\w+", "", x)
# remove tabs
x = gsub("[ |\t]{2,}", "", x)
# remove blank spaces at the beginning
x = gsub("^ ", "", x)
# remove blank spaces at the end
x = gsub(" $", "", x)
return(x)
}
clean_tweet <- clean.text(tweets.txt)
inspect(clean_tweet)
wordsToRemove <- c(stopwords('en'), 'tco', 'https')
tweets <- Corpus(VectorSource(clean_tweet))
clean_tweet <- tm_map(tweets, removeWords, wordsToRemove)
dtm <- TermDocumentMatrix(clean_tweet, control = list(wordLengths = c(1, Inf)))
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "lightblue",
minRotation = -pi/4, maxRotation = -pi/4, size = 0.5)
knitr::opts_chunk$set(echo = TRUE)
library(twitteR)
library(ROAuth)
library(stringr)
library(tm)
library(wordcloud2)
library(tidytext)
library(tidyverse)
library(dplyr)
consumer_key <- "mftPn6qtOfCL46fzJY3pTeVI4"   #Your Consumer Key (API Key)
consumer_secret <- "x5D1VIlV1O4TkZPshj31j88CutI8X2eSfgryy0pdMpDJjUTcKK"  #Your Consumer Secret (API Secret)
access_token <- "2924790128-WORx3u9bTwDqAYr6zSanDnDlafLaSHBdzVKYZxC"  # Your Access Token
access_secret <- "aMFTRClgP3Q2K0pJ21p1fromq1OWx4cS1hU6i4qJyqpZb"    #Your Access Token Secret
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#Query a hashtag
#tweets <- searchTwitter("#rstats",n=3000,lang="en", resultType = "popular")
OR
#Query a hashtag
#tweets <- searchTwitter("#rstats",n=3000,lang="en", resultType = "popular")
#OR
#Query a user you follow or yourself
tweets <- userTimeline("dsilvadeepal",n=3200,includeRts = FALSE)
tweets.txt <- sapply(tweets, function(t)t$getText())
tweets.txt <- sapply(tweets, function(t)t$getText())
tweets.txt <- str_replace_all(tweets.txt,"[^[:graph:]]", " ")
clean.text = function(x)
{
x = tolower(x)                   # tolower
x = gsub("rt", "", x)            # remove rt
x = gsub("@\\w+", "", x)         # remove at
x = gsub("[[:punct:]]", "", x)   # remove punctuation
x = gsub("[[:digit:]]", "", x)   # remove numbers
x = gsub("http\\w+", "", x)      # remove links http
x = gsub("[ |\t]{2,}", "", x)    # remove tabs
x = gsub("^ ", "", x)            # remove blank spaces at the beginning
x = gsub(" $", "", x)            # remove blank spaces at the end
return(x)
}
clean_tweet <- clean.text(tweets.txt)
tweets <- Corpus(VectorSource(clean_tweet))
wordsToRemove <- c(stopwords('en'), 'tco', 'https')
clean_tweet <- tm_map(tweets, removeWords, wordsToRemove)
dtm <- TermDocumentMatrix(clean_tweet, control = list(wordLengths = c(1, Inf)))
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)   #inspect our word list and remove any irrelevant words in the stopwords step above
wordcloud2(d, shape = "star", color="random-dark", backgroundColor = "lightblue",
minRotation = -pi/4, maxRotation = -pi/4, size = 0.5)
knit_with_parameters('C:/Users/dsilv/Desktop/Learning/Data Science/Data-Science/Projects/Twitter Wordcloud.Rmd')
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
setwd("C:/Users/dsil/Desktop/Learning/Data Science/Tidy-Tuesday-Projects/Global causes of mortality - Week 3")
setwd("C:/Users/dsil/Desktop/Learning/Data Science/Tidy-Tuesday-Projects/Global causes of mortality - Week 3/")
setwd("C:/Users/dsil/Desktop/Learning/Data Science/Tidy-Tuesday-Projects")
setwd("C:/Users/dsil/Desktop/Learning/Data Science")
setwd("C:/Users/dsilv/Desktop/Learning/Data Science/Tidy-Tuesday-Projects/Global causes of mortality - Week 3")
install.packages("janitor")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(janitor)
mortality <- read_xlsx("global_mortality.xlsx")
mortality.long <- mortality %>%
clean_names() %>%
gather(cause, percent, 4:35) %>%
mutate(cause = as.factor(cause), country = as.factor(country), country_code = as.factor(country_code))
library(tidyverse)
library(readxl)
library(janitor)
mortality <- read_xlsx("global_mortality.xlsx")
mortality.long <- mortality %>%
clean_names() %>%
gather(cause, percent, 4:35) %>%
mutate(cause = as.factor(cause), country = as.factor(country), country_code = as.factor(country_code))
library(tidyverse)
library(readxl)
library(janitor)
mortality <- read_excel("global_mortality.xlsx")
mortality.long <- mortality %>%
clean_names() %>%
gather(cause, percent, 4:35) %>%
mutate(cause = as.factor(cause), country = as.factor(country), country_code = as.factor(country_code))
library(dplyr)
mortality.long <- mortality %>%
clean_names() %>%
gather(cause, percent, 4:35) %>%
mutate(cause = as.factor(cause), country = as.factor(country), country_code = as.factor(country_code))
View(mortality)
mortality.long <- mortality %>%
clean_names()
mortality <- read_excel("global_mortality.xlsx")
names(mortality) <- names(mortality) %>%
gsub("\\s\\(%\\)","", .)
mortality.long <- mortality %>%
gather(cause, percent, 4:35) %>%
mutate(cause = as.factor(cause), country = as.factor(country), country_code = as.factor(country_code))
summary(mortality.long)
table(mortality.long$country, mortality.long$country_code == "")
